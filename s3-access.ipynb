{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f9e59dc",
   "metadata": {},
   "source": [
    "# Demonstration of S3 in-region access to Earthdata Cloud data\n",
    "In this notebook we will demonstrate how you can find cloud-hosted data within EDC and access that data using AWS' S3 API for in-region compute.\n",
    "EDC data is hosted in us-west-2. In order to run this notebook you need the following,\n",
    "- An EDL account (you can gain one at urs.earthdata.nasa.gov)\n",
    "- A notebook server running in us-west-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6920b12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request, parse\n",
    "from http.cookiejar import CookieJar\n",
    "import getpass\n",
    "import netrc\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b903423",
   "metadata": {},
   "source": [
    "## Registration and authentication\n",
    "\n",
    "In order to access EDC data you need to register with Earthdata Login (EDL) and obtain EDL credentials for your data access.\n",
    "\n",
    "This function below will allow Python scripts to log into the Earthdata Login application programmatically. To avoid being prompted for credentials every time you run and also allow clients such as curl to log in, you can add the following to a .netrc (_netrc on Windows) file in your home directory:\n",
    "\n",
    "machine urs.earthdata.nasa.gov\n",
    "    login <your username>\n",
    "    password <your password>\n",
    "Make sure that this file is only readable by the current user or you will receive an error stating \"netrc access too permissive.\"\n",
    "\n",
    "$ chmod 0600 ~/.netrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e20e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_earthdata_login_auth(endpoint):\n",
    "    \"\"\"\n",
    "    Set up the request library so that it authenticates against the given Earthdata Login\n",
    "    endpoint and is able to track cookies between requests.  This looks in the .netrc file \n",
    "    first and if no credentials are found, it prompts for them.\n",
    "\n",
    "    Valid endpoints include:\n",
    "        uat.urs.earthdata.nasa.gov - Earthdata Login UAT\n",
    "        urs.earthdata.nasa.gov - Earthdata Login production\n",
    "    \"\"\"\n",
    "    try:\n",
    "        username, _, password = netrc.netrc().authenticators(endpoint)\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        # FileNotFound = There's no .netrc file\n",
    "        # TypeError = The endpoint isn't in the netrc file, causing the above to try unpacking None\n",
    "        print('Please provide your Earthdata Login credentials to allow data access')\n",
    "        print('Your credentials will only be passed to %s and will not be exposed in Jupyter' % (endpoint))\n",
    "        username = input('Username:')\n",
    "        password = getpass.getpass()\n",
    "\n",
    "    manager = request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    manager.add_password(None, endpoint, username, password)\n",
    "    auth = request.HTTPBasicAuthHandler(manager)\n",
    "\n",
    "    jar = CookieJar()\n",
    "    processor = request.HTTPCookieProcessor(jar)\n",
    "    opener = request.build_opener(auth, processor)\n",
    "    request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad03ccd",
   "metadata": {},
   "source": [
    "Let's set up our EDL authentication against the producton environment at urs.earthdata.nasa.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8b9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_earthdata_login_auth('urs.earthdata.nasa.gov')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2653ac45",
   "metadata": {},
   "source": [
    "## Data discovery via the Common Metadata Repository (CMR)\n",
    "### Step 1: Collection/Dataset discovery.\n",
    "We can search for collections of interest in our cloud provider POCLOUD using CMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9872f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://cmr.earthdata.nasa.gov/search/collections.json', params={'provider': 'POCLOUD'})\n",
    "results = json.loads(response.content)\n",
    "\n",
    "concept_id = results[\"feed\"][\"entry\"][0][\"id\"]\n",
    "print(\"Unique identifier of collection: \" + concept_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30401851",
   "metadata": {},
   "source": [
    "Note that this collection metadata describes several things you need to know about accessing EDC data via S3\n",
    "In order to use EDC S3 we need to know the following\n",
    "- the region the data is housed in\n",
    "- how to obtain AWS STS credentials (ie. the STS credentials endpoint and documentation\n",
    "EDC needs to metric each data access in terms of the user performing the access. This is done by linking your EDL user name to an STS role. The STS credential endpoint does that by asking for your EDL credentials and returning temporay STS credentials that you can use to set up an AWS S3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08424dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://cmr.earthdata.nasa.gov/search/concepts/' + concept_id + '.umm-json')\n",
    "results = json.loads(response.content)\n",
    "aws_region = results[\"DirectDistributionInformation\"][\"Region\"]\n",
    "print('AWS region: ' + aws_region)\n",
    "sts_endpoint = results[\"DirectDistributionInformation\"][\"S3CredentialsAPIEndpoint\"]\n",
    "print('AWS STS endpoint: ' + sts_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccfff72",
   "metadata": {},
   "source": [
    "### Step 2: Granule/file discovery.\n",
    "Using the unique identifier for the first collection returned, we can search for granules and obtain one or more S3 urls locating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b55894",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://cmr.earthdata.nasa.gov/search/granules.json', params={'concept_id': concept_id})\n",
    "results = json.loads(response.content)\n",
    "\n",
    "links = results[\"feed\"][\"entry\"][0][\"links\"]\n",
    "for link in links:\n",
    "    if link['rel'] == \"http://esipfed.org/ns/fedsearch/1.1/s3#\":\n",
    "        url = link['href']\n",
    "        break;\n",
    "print(\"S3 URL for data: \" + url)\n",
    "o = urlparse(url, allow_fragments=False)\n",
    "\n",
    "bucket = o.netloc\n",
    "\n",
    "key = o.path.lstrip('/')\n",
    "print(\"S3 bucket: \" + bucket)\n",
    "print(\"S3 key: \" + key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355d2455",
   "metadata": {},
   "source": [
    "## Accessing data\n",
    "Now we have found the location of the data, we need to leverage the S3 API to access it. \n",
    "### Step 1: Obtain AWS STS credentials.\n",
    "EDC requires AWS STS credentials for data access the STS endpoint allows us to use our EDL credentials to obtain them.\n",
    "Our EDL credentials are in our https session so the STS endpoint will recognize that and use them to return us STS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(sts_endpoint)\n",
    "creds = json.loads(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576ca316",
   "metadata": {},
   "source": [
    "### Step 2: Accessing the data via the AWS S3 API\n",
    "\n",
    "In order to use the S3 API, supply the access key id, secret access key and session token you retrieved from the STS credentials endpoint.\n",
    "Note: STS credentials are valid for one hour!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=creds[\"accessKeyId\"],\n",
    "        aws_secret_access_key=creds[\"secretAccessKey\"],\n",
    "        aws_session_token=creds[\"sessionToken\"]\n",
    "    )\n",
    "\n",
    "print(\"STS credentials expire on \" + creds[\"expiration\"])\n",
    "\n",
    "# Get your data\n",
    "response = client.get_object(\n",
    "    Bucket=bucket,\n",
    "    Key=key,\n",
    ")\n",
    "\n",
    "print(\"You just accessed \" + response[\"ResponseMetadata\"][\"HTTPHeaders\"][\"content-length\"] + \" bytes of data in-region via the AWS S3 API.\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
