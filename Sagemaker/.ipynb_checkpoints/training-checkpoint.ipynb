{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "\n",
    "training_image = image_uris.retrieve(\n",
    "    region=boto3.Session().region_name, framework=\"image-classification\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(profile_name='sandbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this training, we will use 18 layers\n",
    "num_layers = \"18\"\n",
    "# we need to specify the input image shape for the training data\n",
    "image_shape = \"1000,1000\"\n",
    "# we also need to specify the number of training samples in the training set\n",
    "# for caltech it is 15420\n",
    "num_training_samples = \"100\"\n",
    "# specify the number of output classes\n",
    "num_classes = \"2\"\n",
    "# batch size for training\n",
    "mini_batch_size = \"50\"\n",
    "# number of epochs\n",
    "epochs = \"2\"\n",
    "# learning rate\n",
    "learning_rate = \"0.01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model\n",
    "In order to train our model we need labelled data. The labelling is supplied by an augmented manifest file (see http-data-preparation.ipynb) located in an S3 bucket that you own. \n",
    "\n",
    "That file labels data resident in a bucket in the EDC (which you do not own). For example, a line in that file would look like this:\n",
    "\n",
    "`{\"source-ref\": \"s3://lp-prod-public/HLSL30.020/HLS.L30.T01FBF.2021104T213801.v2.0/HLS.L30.T01FBF.2021104T213801.v2.0.jpg\", \"class\": \"1\"}`\n",
    "\n",
    "Indicating that the browse image in `lp-prod-public` is a cloudy image.\n",
    "\n",
    "We supply a manifest file for both training and validation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: task-4-imageclassification--2022-10-14-19-34-34\n",
      "\n",
      "Input Data Location: {'AlgorithmSpecification': {'TrainingImage': '433757028032.dkr.ecr.us-west-2.amazonaws.com/image-classification:1', 'TrainingInputMode': 'Pipe'}, 'RoleArn': 'arn:aws:iam::286258962151:role/service-role/AmazonSageMaker-ExecutionRole-20210903T160549', 'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-west-2-286258962151/task-4-imageclassification/output'}, 'ResourceConfig': {'InstanceCount': 1, 'InstanceType': 'ml.p3.2xlarge', 'VolumeSizeInGB': 50}, 'TrainingJobName': 'task-4-imageclassification--2022-10-14-19-34-34', 'HyperParameters': {'num_layers': '18', 'num_training_samples': '100', 'num_classes': '2', 'mini_batch_size': '50', 'epochs': '2', 'learning_rate': '0.01'}, 'StoppingCondition': {'MaxRuntimeInSeconds': 360000}, 'InputDataConfig': [{'ChannelName': 'train', 'DataSource': {'S3DataSource': {'AttributeNames': ['source-ref', 'class'], 'S3DataType': 'AugmentedManifestFile', 'S3Uri': 's3://dug-cloud-manifest/cloud_training.json', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'application/x-recordio', 'CompressionType': 'None', 'RecordWrapperType': 'RecordIO'}, {'ChannelName': 'validation', 'DataSource': {'S3DataSource': {'AttributeNames': ['source-ref', 'class'], 'S3DataType': 'AugmentedManifestFile', 'S3Uri': 's3://dug-cloud-manifest/cloud_validation.json', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'application/x-recordio', 'CompressionType': 'None', 'RecordWrapperType': 'RecordIO'}]}\n"
     ]
    }
   ],
   "source": [
    "job_name_prefix = \"task-4-imageclassification\"\n",
    "job_name = job_name_prefix + \"-\" + time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "training_params = {\n",
    "    \"AlgorithmSpecification\": {\"TrainingImage\": training_image, \"TrainingInputMode\": \"Pipe\"},\n",
    "    \"RoleArn\": role,\n",
    "    \"OutputDataConfig\": {\"S3OutputPath\": \"s3://{}/{}/output\".format(bucket, job_name_prefix)},\n",
    "    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.p3.2xlarge\", \"VolumeSizeInGB\": 50},\n",
    "    \"TrainingJobName\": job_name,\n",
    "    \"HyperParameters\": {\n",
    "        \"num_layers\": str(num_layers),\n",
    "        \"num_training_samples\": str(num_training_samples),\n",
    "        \"num_classes\": str(num_classes),\n",
    "        \"mini_batch_size\": str(mini_batch_size),\n",
    "        \"epochs\": str(epochs),\n",
    "        \"learning_rate\": str(learning_rate)\n",
    "    },\n",
    "    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 360000},\n",
    "    \"InputDataConfig\": [\n",
    "        {\n",
    "            \"ChannelName\": \"train\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"AttributeNames\": [\"source-ref\", \"class\"],\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": \"s3://dug-cloud-manifest/cloud_training_3.json\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"RecordIO\"\n",
    "        },\n",
    "        {\n",
    "            \"ChannelName\": \"validation\",\n",
    "            \"DataSource\": {\n",
    "                \"S3DataSource\": {\n",
    "                    \"AttributeNames\": [\"source-ref\", \"class\"],\n",
    "                    \"S3DataType\": \"AugmentedManifestFile\",\n",
    "                    \"S3Uri\": \"s3://dug-cloud-manifest/cloud_validation_3.json\",\n",
    "                    \"S3DataDistributionType\": \"FullyReplicated\"\n",
    "                }\n",
    "            },\n",
    "            \"ContentType\": \"application/x-recordio\",\n",
    "            \"CompressionType\": \"None\",\n",
    "            \"RecordWrapperType\": \"RecordIO\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "    \n",
    "print(\"Training job name: {}\".format(job_name))\n",
    "print(\n",
    "    \"\\nInput Data Location: {}\".format(\n",
    "#        training_params[\"InputDataConfig\"][0][\"DataSource\"][\"S3DataSource\"]\n",
    "#    )\n",
    "        training_params\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job current status: InProgress\n",
      "Training failed to start\n",
      "Training failed with the following error: ClientError: The number of valid input images must be bigger or equal to the mini_batch_size., exit code: 2\n"
     ]
    }
   ],
   "source": [
    "# create the Amazon SageMaker training job\n",
    "sagemaker = session.client(service_name=\"sagemaker\")\n",
    "sagemaker.create_training_job(**training_params)\n",
    "\n",
    "# confirm that the training job has started\n",
    "status = sagemaker.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n",
    "print(\"Training job current status: {}\".format(status))\n",
    "\n",
    "try:\n",
    "    # wait for the job to finish and report the ending status\n",
    "    sagemaker.get_waiter(\"training_job_completed_or_stopped\").wait(TrainingJobName=job_name)\n",
    "    training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "    status = training_info[\"TrainingJobStatus\"]\n",
    "    print(\"Training job ended with status: \" + status)\n",
    "except:\n",
    "    print(\"Training failed to start\")\n",
    "    # if exception is raised, that means it has failed\n",
    "    message = sagemaker.describe_training_job(TrainingJobName=job_name)[\"FailureReason\"]\n",
    "    print(\"Training failed with the following error: {}\".format(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job ended with status: Failed\n"
     ]
    }
   ],
   "source": [
    "training_info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "status = training_info[\"TrainingJobStatus\"]\n",
    "print(\"Training job ended with status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DEMO-full-image-classification-model\" + time.strftime(\n",
    "    \"-%Y-%m-%d-%H-%M-%S\", time.gmtime()\n",
    ")\n",
    "print(model_name)\n",
    "info = sagemaker.describe_training_job(TrainingJobName=job_name)\n",
    "model_data = info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "print(model_data)\n",
    "\n",
    "hosting_image = image_uris.retrieve(\n",
    "    region=session.region_name, framework=\"image-classification\"\n",
    ")\n",
    "\n",
    "primary_container = {\n",
    "    \"Image\": hosting_image,\n",
    "    \"ModelDataUrl\": model_data,\n",
    "}\n",
    "\n",
    "create_model_response = sagemaker.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=primary_container\n",
    ")\n",
    "\n",
    "print(create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuring a model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "endpoint_config_name = job_name_prefix + \"-epc-\" + timestamp\n",
    "endpoint_config_response = sagemaker.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.m4.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint configuration name: {}\".format(endpoint_config_name))\n",
    "print(\"Endpoint configuration arn:  {}\".format(endpoint_config_response[\"EndpointConfigArn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an image classification endpoint from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = time.strftime(\"-%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "endpoint_name = job_name_prefix + \"-ep-\" + timestamp\n",
    "print(\"Endpoint name: {}\".format(endpoint_name))\n",
    "\n",
    "endpoint_params = {\n",
    "    \"EndpointName\": endpoint_name,\n",
    "    \"EndpointConfigName\": endpoint_config_name,\n",
    "}\n",
    "endpoint_response = sagemaker.create_endpoint(**endpoint_params)\n",
    "print(\"EndpointArn = {}\".format(endpoint_response[\"EndpointArn\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the image classification endpoint\n",
    "We can feed the endpoint with a browse image and ask it whether that image is cloudy or clear.\n",
    "We render the image, feed it to the endpoint and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from PIL import Image as Im\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "object_categories = [\n",
    "    \"clear\",\n",
    "    \"cloudy\"\n",
    "]\n",
    "runtime = session.client(service_name=\"runtime.sagemaker\")\n",
    "\n",
    "images = (\"test_1.jpg\", \"test_2.jpg\", \"test_3.jpg\", \"test_4.jpg\", \"test_5.jpg\")\n",
    "imageIterator = iter(images)\n",
    "\n",
    "def testImage(image):\n",
    "    file_name = \"/tmp/\" + image\n",
    "    s3_client = boto3.client(\"s3\")\n",
    "    s3_client.download_file(\n",
    "        \"dug-cloud-test\",\n",
    "        image,\n",
    "        file_name,\n",
    "    )\n",
    "\n",
    "    # test image\n",
    "    Image(file_name)\n",
    "\n",
    "    img = Im.open(file_name)\n",
    "\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show(imgplot)     \n",
    "    img = open(file_name, 'rb').read()\n",
    "    \n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/x-image\", Body=bytearray(img)\n",
    "    )\n",
    "\n",
    "    result = response[\"Body\"].read()\n",
    "\n",
    "    # result will be in json format and convert it to ndarray\n",
    "    result = json.loads(result)\n",
    "    print(\"Result: \" + str(result))\n",
    "    # the result will output the probabilities for all classes\n",
    "    # find the class with maximum probability and print the class index\n",
    "    index = np.argmax(result)\n",
    "    print(\"This image is \" + object_categories[index])\n",
    "    \n",
    "testImage(next(imageIterator))\n",
    "testImage(next(imageIterator))\n",
    "testImage(next(imageIterator))\n",
    "testImage(next(imageIterator))\n",
    "testImage(next(imageIterator))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage.delete_endpoint(EndpointName=endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
